{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Sequence to Sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helpers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_path = '../data/letters_source.txt'\n",
    "target_path = '../data/letters_target.txt'\n",
    "\n",
    "source_sentences = helpers.load_data(source_path)\n",
    "target_sentences = helpers.load_data(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bsaqq',\n",
       " 'npy',\n",
       " 'lbwuj',\n",
       " 'bqv',\n",
       " 'kial',\n",
       " 'tddam',\n",
       " 'edxpjpg',\n",
       " 'nspv',\n",
       " 'huloz',\n",
       " '']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_sentences[:50].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abqqs',\n",
       " 'npy',\n",
       " 'bjluw',\n",
       " 'bqv',\n",
       " 'aikl',\n",
       " 'addmt',\n",
       " 'degjppx',\n",
       " 'npsv',\n",
       " 'hlouz',\n",
       " '']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_sentences[:50].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example source sequence\n",
      "[[28, 25, 7, 13, 13], [27, 12, 17], [19, 28, 9, 26, 10]]\n",
      "\n",
      "\n",
      "Example target sequence\n",
      "[[7, 28, 13, 13, 25, 3], [27, 12, 17, 3], [28, 10, 19, 26, 9, 3]]\n"
     ]
    }
   ],
   "source": [
    "def extract_character_vocab(data):\n",
    "    special_characters = ['<PAD>', '<UNK>', '<GO>',  '<EOS>']\n",
    "    # <PAD>:补全字符\n",
    "    # <EOS>:解码器端的句子结束标识符\n",
    "    # <UNK>:低频词或者一些未遇到的词等\n",
    "    # <GO>:解码器端的句子起始标识符\n",
    "    \n",
    "    set_characters = set([character for line in data.split('\\n') for character in line])\n",
    "    int_to_vocab = {i:character for i,character in enumerate(special_characters+list(set_characters))}\n",
    "    vocab_to_int = {character:i for i,character in int_to_vocab.items()}\n",
    "    return int_to_vocab, vocab_to_int\n",
    "\n",
    "source_int_to_letter, source_letter_to_int = extract_character_vocab(source_sentences)\n",
    "target_int_to_letter, target_letter_to_int = extract_character_vocab(target_sentences)\n",
    "\n",
    "source_letter_ids = [[source_letter_to_int.get(letter, source_letter_to_int['<UNK>']) for letter in line]\n",
    "                     for line in source_sentences.split('\\n')]\n",
    "target_letter_ids = [[target_letter_to_int.get(letter, target_letter_to_int['<UNK>']) for letter in line]+ [target_letter_to_int['<EOS>']]\n",
    "                     for line in target_sentences.split('\\n')]\n",
    "\n",
    "print(\"Example source sequence\")\n",
    "print(source_letter_ids[:3])\n",
    "print(\"\\n\")\n",
    "print(\"Example target sequence\")\n",
    "print(target_letter_ids[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"letter_dicts.pkl\",'wb') as f:\n",
    "    pickle.dump([source_int_to_letter, source_letter_to_int,target_int_to_letter, target_letter_to_int],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "# Number of Epochs\n",
    "epochs = 60\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "# RNN Size\n",
    "rnn_size = 50\n",
    "# Number of Layers\n",
    "num_layers = 2\n",
    "# Embedding Size\n",
    "encoder_embedding_size = 15\n",
    "decoder_embedding_size = 15\n",
    "# Learning Rate\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input\n",
    "tf.reset_default_graph()\n",
    "inputs_ = tf.placeholder(shape=[None, None],dtype=tf.int32, name=\"inputs\")\n",
    "targets_ = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"targets\")\n",
    "lr = tf.placeholder(dtype=tf.float32, name=\"learning_rate\")\n",
    "target_sequence_lengths = tf.placeholder(shape=[None],dtype=tf.int32, name=\"target_sequence_lengths\")\n",
    "max_target_sequence_length = tf.reduce_max(target_sequence_lengths, name='max_target_sequence_length')\n",
    "source_sequence_lengths = tf.placeholder(shape=[None],dtype=tf.int32, name=\"source_sequence_lengths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encoder\n",
    "def encoder_layer(inputs, rnn_size, num_layers, source_sequence_length, \n",
    "            source_vocab_size,encoder_embedding_size):\n",
    "    encoder_input_embed = tf.contrib.layers.embed_sequence(inputs, source_vocab_size, encoder_embedding_size)\n",
    "    def make_cell(rnn_size):\n",
    "        lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "        return lstm\n",
    "    encoder_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) for _ in range(num_layers)])\n",
    "    encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(\n",
    "        encoder_cell, encoder_input_embed, sequence_length=source_sequence_length, dtype=tf.float32, scope=\"encoder_rnn\")\n",
    "    return encoder_outputs, encoder_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_decoder_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    slices = tf.strided_slice(target_data, [0,0],[batch_size,-1],[1,1])\n",
    "    decoder_inputs = tf.concat([tf.fill([batch_size,1], vocab_to_int['<GO>']), slices],1)\n",
    "    return decoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decoder\n",
    "def decoder_layer(decoder_inputs, targets, encoder_state, rnn_size, num_layers, \n",
    "            target_sequence_length, max_target_sequence_length, \n",
    "            target_letter_to_int, decoder_embedding_size):\n",
    "    # Decoder Embdding\n",
    "    target_vocab_size= len(target_letter_to_int)\n",
    "    decoder_embedding = tf.Variable(tf.random_uniform([target_vocab_size, decoder_embedding_size], -1.0, 1.0))\n",
    "    decoder_input_embed = tf.nn.embedding_lookup(decoder_embedding, decoder_inputs)\n",
    "    \n",
    "    # Decoder cell\n",
    "    def make_cell(rnn_size):\n",
    "        lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "        return lstm\n",
    "    decoder_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    # Ouput \n",
    "    ouput_layer = tf.contrib.keras.layers.Dense(\n",
    "        target_vocab_size, kernel_initializer=tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    # Training Decoder\n",
    "    with tf.variable_scope(\"decoder\"):\n",
    "        # Using TrainingHelper to read inputs\n",
    "        train_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "            decoder_input_embed, target_sequence_lengths)\n",
    "        \n",
    "        train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            decoder_cell, train_helper, encoder_state, ouput_layer)\n",
    "        \n",
    "        train_decoder_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            train_decoder, maximum_iterations=max_target_sequence_length, impute_finished=True)\n",
    "    \n",
    "    # Inference\n",
    "    with tf.variable_scope(\"decoder\", reuse=True):\n",
    "        start_tokens = tf.fill([batch_size],target_letter_to_int['<GO>'])\n",
    "        infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "            embedding=decoder_embedding, start_tokens=start_tokens, end_token=target_letter_to_int['<EOS>'])\n",
    "        \n",
    "        infer_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            decoder_cell, infer_helper, encoder_state, ouput_layer)\n",
    "        \n",
    "        infer_decoder_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            infer_decoder, maximum_iterations=max_target_sequence_length, impute_finished=True)\n",
    "    \n",
    "    return train_decoder_outputs, infer_decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_, encoder_final_state = encoder_layer(inputs_, rnn_size, num_layers, source_sequence_lengths, \n",
    "                                       len(source_letter_to_int),encoder_embedding_size)\n",
    "\n",
    "decoder_inputs = process_decoder_input(targets_, target_letter_to_int, batch_size)\n",
    "\n",
    "train_decoder_outputs, infer_decoder_outputs = decoder_layer(decoder_inputs, targets_, encoder_final_state, rnn_size, num_layers,\n",
    "                                                             target_sequence_lengths, max_target_sequence_length, target_letter_to_int, decoder_embedding_size)\n",
    "\n",
    "train_logits = tf.identity(train_decoder_outputs.rnn_output, 'logits')\n",
    "infere_logits = tf.identity(infer_decoder_outputs.sample_id, name='predictions')\n",
    "\n",
    "masks = tf.sequence_mask(target_sequence_lengths, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "with tf.name_scope(\"optimization\"):\n",
    "    cost = tf.contrib.seq2seq.sequence_loss(logits=train_logits, targets = targets_, weights = masks)\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'masks/Cast_1:0' shape=(?, ?) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../data/source_batch.png\" />\n",
    "\n",
    "<img src=\"../data/target_batch.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence_length = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int]*(max_sentence_length-len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(targets, sources, batch_size, source_pad_int, target_pad_int):\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        sources_batch = sources[batch_i * batch_size:(batch_i+1) * batch_size]\n",
    "        targets_batch = targets[batch_i * batch_size:(batch_i+1) * batch_size]\n",
    "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "        \n",
    "        pad_targets_lengths = []\n",
    "        for target in pad_targets_batch:\n",
    "            pad_targets_lengths.append(len(target))\n",
    "        \n",
    "        pad_source_lengths = []\n",
    "        for source in pad_sources_batch:\n",
    "            pad_source_lengths.append(len(source))\n",
    "        \n",
    "        yield pad_targets_batch, pad_sources_batch, pad_targets_lengths, pad_source_lengths      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/60 Batch   20/77 - Loss:  2.401  - Validation loss:  2.428\n",
      "Epoch   1/60 Batch   40/77 - Loss:  2.192  - Validation loss:  2.145\n",
      "Epoch   1/60 Batch   60/77 - Loss:  1.942  - Validation loss:  1.989\n",
      "Epoch   2/60 Batch   20/77 - Loss:  1.687  - Validation loss:  1.765\n",
      "Epoch   2/60 Batch   40/77 - Loss:  1.692  - Validation loss:  1.652\n",
      "Epoch   2/60 Batch   60/77 - Loss:  1.528  - Validation loss:  1.554\n",
      "Epoch   3/60 Batch   20/77 - Loss:  1.384  - Validation loss:  1.455\n",
      "Epoch   3/60 Batch   40/77 - Loss:  1.460  - Validation loss:  1.429\n",
      "Epoch   3/60 Batch   60/77 - Loss:  1.374  - Validation loss:  1.398\n",
      "Epoch   4/60 Batch   20/77 - Loss:  1.273  - Validation loss:  1.342\n",
      "Epoch   4/60 Batch   40/77 - Loss:  1.331  - Validation loss:  1.287\n",
      "Epoch   4/60 Batch   60/77 - Loss:  1.202  - Validation loss:  1.240\n",
      "Epoch   5/60 Batch   20/77 - Loss:  1.093  - Validation loss:  1.164\n",
      "Epoch   5/60 Batch   40/77 - Loss:  1.148  - Validation loss:  1.128\n",
      "Epoch   5/60 Batch   60/77 - Loss:  1.024  - Validation loss:  1.091\n",
      "Epoch   6/60 Batch   20/77 - Loss:  0.958  - Validation loss:  1.037\n",
      "Epoch   6/60 Batch   40/77 - Loss:  1.017  - Validation loss:  1.007\n",
      "Epoch   6/60 Batch   60/77 - Loss:  0.912  - Validation loss:  0.980\n",
      "Epoch   7/60 Batch   20/77 - Loss:  0.840  - Validation loss:  0.931\n",
      "Epoch   7/60 Batch   40/77 - Loss:  0.889  - Validation loss:  0.896\n",
      "Epoch   7/60 Batch   60/77 - Loss:  0.798  - Validation loss:  0.862\n",
      "Epoch   8/60 Batch   20/77 - Loss:  0.711  - Validation loss:  0.804\n",
      "Epoch   8/60 Batch   40/77 - Loss:  0.762  - Validation loss:  0.774\n",
      "Epoch   8/60 Batch   60/77 - Loss:  0.687  - Validation loss:  0.747\n",
      "Epoch   9/60 Batch   20/77 - Loss:  0.596  - Validation loss:  0.697\n",
      "Epoch   9/60 Batch   40/77 - Loss:  0.663  - Validation loss:  0.670\n",
      "Epoch   9/60 Batch   60/77 - Loss:  0.592  - Validation loss:  0.639\n",
      "Epoch  10/60 Batch   20/77 - Loss:  0.497  - Validation loss:  0.591\n",
      "Epoch  10/60 Batch   40/77 - Loss:  0.567  - Validation loss:  0.569\n",
      "Epoch  10/60 Batch   60/77 - Loss:  0.505  - Validation loss:  0.544\n",
      "Epoch  11/60 Batch   20/77 - Loss:  0.418  - Validation loss:  0.509\n",
      "Epoch  11/60 Batch   40/77 - Loss:  0.505  - Validation loss:  0.493\n",
      "Epoch  11/60 Batch   60/77 - Loss:  0.436  - Validation loss:  0.472\n",
      "Epoch  12/60 Batch   20/77 - Loss:  0.353  - Validation loss:  0.444\n",
      "Epoch  12/60 Batch   40/77 - Loss:  0.424  - Validation loss:  0.431\n",
      "Epoch  12/60 Batch   60/77 - Loss:  0.378  - Validation loss:  0.411\n",
      "Epoch  13/60 Batch   20/77 - Loss:  0.300  - Validation loss:  0.390\n",
      "Epoch  13/60 Batch   40/77 - Loss:  0.365  - Validation loss:  0.378\n",
      "Epoch  13/60 Batch   60/77 - Loss:  0.334  - Validation loss:  0.363\n",
      "Epoch  14/60 Batch   20/77 - Loss:  0.258  - Validation loss:  0.340\n",
      "Epoch  14/60 Batch   40/77 - Loss:  0.325  - Validation loss:  0.333\n",
      "Epoch  14/60 Batch   60/77 - Loss:  0.290  - Validation loss:  0.315\n",
      "Epoch  15/60 Batch   20/77 - Loss:  0.224  - Validation loss:  0.298\n",
      "Epoch  15/60 Batch   40/77 - Loss:  0.275  - Validation loss:  0.288\n",
      "Epoch  15/60 Batch   60/77 - Loss:  0.251  - Validation loss:  0.278\n",
      "Epoch  16/60 Batch   20/77 - Loss:  0.197  - Validation loss:  0.267\n",
      "Epoch  16/60 Batch   40/77 - Loss:  0.240  - Validation loss:  0.296\n",
      "Epoch  16/60 Batch   60/77 - Loss:  0.408  - Validation loss:  0.336\n",
      "Epoch  17/60 Batch   20/77 - Loss:  0.181  - Validation loss:  0.246\n",
      "Epoch  17/60 Batch   40/77 - Loss:  0.218  - Validation loss:  0.232\n",
      "Epoch  17/60 Batch   60/77 - Loss:  0.203  - Validation loss:  0.225\n",
      "Epoch  18/60 Batch   20/77 - Loss:  0.154  - Validation loss:  0.211\n",
      "Epoch  18/60 Batch   40/77 - Loss:  0.192  - Validation loss:  0.205\n",
      "Epoch  18/60 Batch   60/77 - Loss:  0.182  - Validation loss:  0.200\n",
      "Epoch  19/60 Batch   20/77 - Loss:  0.137  - Validation loss:  0.188\n",
      "Epoch  19/60 Batch   40/77 - Loss:  0.172  - Validation loss:  0.185\n",
      "Epoch  19/60 Batch   60/77 - Loss:  0.164  - Validation loss:  0.179\n",
      "Epoch  20/60 Batch   20/77 - Loss:  0.123  - Validation loss:  0.169\n",
      "Epoch  20/60 Batch   40/77 - Loss:  0.155  - Validation loss:  0.167\n",
      "Epoch  20/60 Batch   60/77 - Loss:  0.148  - Validation loss:  0.162\n",
      "Epoch  21/60 Batch   20/77 - Loss:  0.110  - Validation loss:  0.153\n",
      "Epoch  21/60 Batch   40/77 - Loss:  0.140  - Validation loss:  0.151\n",
      "Epoch  21/60 Batch   60/77 - Loss:  0.134  - Validation loss:  0.147\n",
      "Epoch  22/60 Batch   20/77 - Loss:  0.099  - Validation loss:  0.138\n",
      "Epoch  22/60 Batch   40/77 - Loss:  0.127  - Validation loss:  0.138\n",
      "Epoch  22/60 Batch   60/77 - Loss:  0.121  - Validation loss:  0.133\n",
      "Epoch  23/60 Batch   20/77 - Loss:  0.089  - Validation loss:  0.126\n",
      "Epoch  23/60 Batch   40/77 - Loss:  0.115  - Validation loss:  0.126\n",
      "Epoch  23/60 Batch   60/77 - Loss:  0.109  - Validation loss:  0.122\n",
      "Epoch  24/60 Batch   20/77 - Loss:  0.080  - Validation loss:  0.114\n",
      "Epoch  24/60 Batch   40/77 - Loss:  0.104  - Validation loss:  0.116\n",
      "Epoch  24/60 Batch   60/77 - Loss:  0.099  - Validation loss:  0.112\n",
      "Epoch  25/60 Batch   20/77 - Loss:  0.072  - Validation loss:  0.105\n",
      "Epoch  25/60 Batch   40/77 - Loss:  0.095  - Validation loss:  0.107\n",
      "Epoch  25/60 Batch   60/77 - Loss:  0.089  - Validation loss:  0.103\n",
      "Epoch  26/60 Batch   20/77 - Loss:  0.065  - Validation loss:  0.097\n",
      "Epoch  26/60 Batch   40/77 - Loss:  0.087  - Validation loss:  0.099\n",
      "Epoch  26/60 Batch   60/77 - Loss:  0.081  - Validation loss:  0.095\n",
      "Epoch  27/60 Batch   20/77 - Loss:  0.059  - Validation loss:  0.089\n",
      "Epoch  27/60 Batch   40/77 - Loss:  0.079  - Validation loss:  0.092\n",
      "Epoch  27/60 Batch   60/77 - Loss:  0.074  - Validation loss:  0.088\n",
      "Epoch  28/60 Batch   20/77 - Loss:  0.053  - Validation loss:  0.083\n",
      "Epoch  28/60 Batch   40/77 - Loss:  0.072  - Validation loss:  0.086\n",
      "Epoch  28/60 Batch   60/77 - Loss:  0.068  - Validation loss:  0.082\n",
      "Epoch  29/60 Batch   20/77 - Loss:  0.049  - Validation loss:  0.077\n",
      "Epoch  29/60 Batch   40/77 - Loss:  0.066  - Validation loss:  0.080\n",
      "Epoch  29/60 Batch   60/77 - Loss:  0.062  - Validation loss:  0.076\n",
      "Epoch  30/60 Batch   20/77 - Loss:  0.044  - Validation loss:  0.072\n",
      "Epoch  30/60 Batch   40/77 - Loss:  0.061  - Validation loss:  0.075\n",
      "Epoch  30/60 Batch   60/77 - Loss:  0.057  - Validation loss:  0.071\n",
      "Epoch  31/60 Batch   20/77 - Loss:  0.040  - Validation loss:  0.068\n",
      "Epoch  31/60 Batch   40/77 - Loss:  0.056  - Validation loss:  0.070\n",
      "Epoch  31/60 Batch   60/77 - Loss:  0.053  - Validation loss:  0.066\n",
      "Epoch  32/60 Batch   20/77 - Loss:  0.037  - Validation loss:  0.063\n",
      "Epoch  32/60 Batch   40/77 - Loss:  0.051  - Validation loss:  0.066\n",
      "Epoch  32/60 Batch   60/77 - Loss:  0.049  - Validation loss:  0.062\n",
      "Epoch  33/60 Batch   20/77 - Loss:  0.034  - Validation loss:  0.059\n",
      "Epoch  33/60 Batch   40/77 - Loss:  0.047  - Validation loss:  0.062\n",
      "Epoch  33/60 Batch   60/77 - Loss:  0.045  - Validation loss:  0.058\n",
      "Epoch  34/60 Batch   20/77 - Loss:  0.031  - Validation loss:  0.056\n",
      "Epoch  34/60 Batch   40/77 - Loss:  0.043  - Validation loss:  0.058\n",
      "Epoch  34/60 Batch   60/77 - Loss:  0.042  - Validation loss:  0.055\n",
      "Epoch  35/60 Batch   20/77 - Loss:  0.029  - Validation loss:  0.052\n",
      "Epoch  35/60 Batch   40/77 - Loss:  0.040  - Validation loss:  0.054\n",
      "Epoch  35/60 Batch   60/77 - Loss:  0.039  - Validation loss:  0.051\n",
      "Epoch  36/60 Batch   20/77 - Loss:  0.027  - Validation loss:  0.049\n",
      "Epoch  36/60 Batch   40/77 - Loss:  0.037  - Validation loss:  0.051\n",
      "Epoch  36/60 Batch   60/77 - Loss:  0.036  - Validation loss:  0.048\n",
      "Epoch  37/60 Batch   20/77 - Loss:  0.025  - Validation loss:  0.046\n",
      "Epoch  37/60 Batch   40/77 - Loss:  0.034  - Validation loss:  0.048\n",
      "Epoch  37/60 Batch   60/77 - Loss:  0.033  - Validation loss:  0.045\n",
      "Epoch  38/60 Batch   20/77 - Loss:  0.023  - Validation loss:  0.043\n",
      "Epoch  38/60 Batch   40/77 - Loss:  0.032  - Validation loss:  0.045\n",
      "Epoch  38/60 Batch   60/77 - Loss:  0.031  - Validation loss:  0.042\n",
      "Epoch  39/60 Batch   20/77 - Loss:  0.022  - Validation loss:  0.040\n",
      "Epoch  39/60 Batch   40/77 - Loss:  0.030  - Validation loss:  0.042\n",
      "Epoch  39/60 Batch   60/77 - Loss:  0.029  - Validation loss:  0.039\n",
      "Epoch  40/60 Batch   20/77 - Loss:  0.020  - Validation loss:  0.038\n",
      "Epoch  40/60 Batch   40/77 - Loss:  0.028  - Validation loss:  0.039\n",
      "Epoch  40/60 Batch   60/77 - Loss:  0.027  - Validation loss:  0.037\n",
      "Epoch  41/60 Batch   20/77 - Loss:  0.019  - Validation loss:  0.036\n",
      "Epoch  41/60 Batch   40/77 - Loss:  0.026  - Validation loss:  0.037\n",
      "Epoch  41/60 Batch   60/77 - Loss:  0.025  - Validation loss:  0.035\n",
      "Epoch  42/60 Batch   20/77 - Loss:  0.017  - Validation loss:  0.034\n",
      "Epoch  42/60 Batch   40/77 - Loss:  0.024  - Validation loss:  0.034\n",
      "Epoch  42/60 Batch   60/77 - Loss:  0.023  - Validation loss:  0.033\n",
      "Epoch  43/60 Batch   20/77 - Loss:  0.016  - Validation loss:  0.032\n",
      "Epoch  43/60 Batch   40/77 - Loss:  0.023  - Validation loss:  0.032\n",
      "Epoch  43/60 Batch   60/77 - Loss:  0.022  - Validation loss:  0.031\n",
      "Epoch  44/60 Batch   20/77 - Loss:  0.015  - Validation loss:  0.030\n",
      "Epoch  44/60 Batch   40/77 - Loss:  0.021  - Validation loss:  0.030\n",
      "Epoch  44/60 Batch   60/77 - Loss:  0.021  - Validation loss:  0.029\n",
      "Epoch  45/60 Batch   20/77 - Loss:  0.014  - Validation loss:  0.028\n",
      "Epoch  45/60 Batch   40/77 - Loss:  0.019  - Validation loss:  0.028\n",
      "Epoch  45/60 Batch   60/77 - Loss:  0.020  - Validation loss:  0.028\n",
      "Epoch  46/60 Batch   20/77 - Loss:  0.013  - Validation loss:  0.027\n",
      "Epoch  46/60 Batch   40/77 - Loss:  0.018  - Validation loss:  0.026\n",
      "Epoch  46/60 Batch   60/77 - Loss:  0.019  - Validation loss:  0.028\n",
      "Epoch  47/60 Batch   20/77 - Loss:  0.012  - Validation loss:  0.025\n",
      "Epoch  47/60 Batch   40/77 - Loss:  0.017  - Validation loss:  0.025\n",
      "Epoch  47/60 Batch   60/77 - Loss:  0.017  - Validation loss:  0.027\n",
      "Epoch  48/60 Batch   20/77 - Loss:  0.011  - Validation loss:  0.023\n",
      "Epoch  48/60 Batch   40/77 - Loss:  0.015  - Validation loss:  0.024\n",
      "Epoch  48/60 Batch   60/77 - Loss:  0.015  - Validation loss:  0.025\n",
      "Epoch  49/60 Batch   20/77 - Loss:  0.011  - Validation loss:  0.023\n",
      "Epoch  49/60 Batch   40/77 - Loss:  0.015  - Validation loss:  0.023\n",
      "Epoch  49/60 Batch   60/77 - Loss:  0.014  - Validation loss:  0.022\n",
      "Epoch  50/60 Batch   20/77 - Loss:  0.010  - Validation loss:  0.021\n",
      "Epoch  50/60 Batch   40/77 - Loss:  0.014  - Validation loss:  0.023\n",
      "Epoch  50/60 Batch   60/77 - Loss:  0.014  - Validation loss:  0.021\n",
      "Epoch  51/60 Batch   20/77 - Loss:  0.010  - Validation loss:  0.021\n",
      "Epoch  51/60 Batch   40/77 - Loss:  0.014  - Validation loss:  0.040\n",
      "Epoch  51/60 Batch   60/77 - Loss:  0.017  - Validation loss:  0.032\n",
      "Epoch  52/60 Batch   20/77 - Loss:  0.010  - Validation loss:  0.024\n",
      "Epoch  52/60 Batch   40/77 - Loss:  0.014  - Validation loss:  0.022\n",
      "Epoch  52/60 Batch   60/77 - Loss:  0.034  - Validation loss:  0.039\n",
      "Epoch  53/60 Batch   20/77 - Loss:  0.011  - Validation loss:  0.022\n",
      "Epoch  53/60 Batch   40/77 - Loss:  0.013  - Validation loss:  0.020\n",
      "Epoch  53/60 Batch   60/77 - Loss:  0.013  - Validation loss:  0.020\n",
      "Epoch  54/60 Batch   20/77 - Loss:  0.009  - Validation loss:  0.017\n",
      "Epoch  54/60 Batch   40/77 - Loss:  0.011  - Validation loss:  0.019\n",
      "Epoch  54/60 Batch   60/77 - Loss:  0.011  - Validation loss:  0.017\n",
      "Epoch  55/60 Batch   20/77 - Loss:  0.008  - Validation loss:  0.018\n",
      "Epoch  55/60 Batch   40/77 - Loss:  0.010  - Validation loss:  0.018\n",
      "Epoch  55/60 Batch   60/77 - Loss:  0.010  - Validation loss:  0.017\n",
      "Epoch  56/60 Batch   20/77 - Loss:  0.007  - Validation loss:  0.017\n",
      "Epoch  56/60 Batch   40/77 - Loss:  0.009  - Validation loss:  0.016\n",
      "Epoch  56/60 Batch   60/77 - Loss:  0.009  - Validation loss:  0.016\n",
      "Epoch  57/60 Batch   20/77 - Loss:  0.006  - Validation loss:  0.015\n",
      "Epoch  57/60 Batch   40/77 - Loss:  0.008  - Validation loss:  0.015\n",
      "Epoch  57/60 Batch   60/77 - Loss:  0.008  - Validation loss:  0.015\n",
      "Epoch  58/60 Batch   20/77 - Loss:  0.006  - Validation loss:  0.014\n",
      "Epoch  58/60 Batch   40/77 - Loss:  0.008  - Validation loss:  0.014\n",
      "Epoch  58/60 Batch   60/77 - Loss:  0.008  - Validation loss:  0.014\n",
      "Epoch  59/60 Batch   20/77 - Loss:  0.005  - Validation loss:  0.013\n",
      "Epoch  59/60 Batch   40/77 - Loss:  0.007  - Validation loss:  0.013\n",
      "Epoch  59/60 Batch   60/77 - Loss:  0.007  - Validation loss:  0.014\n",
      "Epoch  60/60 Batch   20/77 - Loss:  0.005  - Validation loss:  0.013\n",
      "Epoch  60/60 Batch   40/77 - Loss:  0.006  - Validation loss:  0.012\n",
      "Epoch  60/60 Batch   60/77 - Loss:  0.007  - Validation loss:  0.013\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "train_source = source_letter_ids[batch_size:]\n",
    "train_target = target_letter_ids[batch_size:]\n",
    "valid_source = source_letter_ids[:batch_size]\n",
    "valid_target = target_letter_ids[:batch_size]\n",
    "(valid_targets_batch, valid_sources_batch, valid_targets_lengths, valid_sources_lengths) = next(get_batches(valid_target, valid_source, batch_size,\n",
    "                           source_letter_to_int['<PAD>'],\n",
    "                           target_letter_to_int['<PAD>']))\n",
    "\n",
    "display_step = 20 # Check training loss after every 20 batches\n",
    "\n",
    "checkpoint = \"../data/letter_checkpoints/best_model.ckpt\" \n",
    "loss_track=[]\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch_i in range(1,epochs+1):\n",
    "        for batch_i, (targets_batch, sources_batch, targets_lengths, sources_lengths) in enumerate(\n",
    "                get_batches(train_target, train_source, batch_size,\n",
    "                           source_letter_to_int['<PAD>'],\n",
    "                           target_letter_to_int['<PAD>'])):\n",
    "            _, loss = sess.run([train_op,cost],feed_dict={\n",
    "                    inputs_:sources_batch,\n",
    "                    targets_:targets_batch,\n",
    "                    lr:learning_rate,\n",
    "                    target_sequence_lengths:targets_lengths,\n",
    "                    source_sequence_lengths:sources_lengths\n",
    "                })\n",
    "            loss_track.append(loss)\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                val_loss = sess.run([cost], feed_dict={\n",
    "                        inputs_:valid_sources_batch,\n",
    "                        targets_:valid_targets_batch,\n",
    "                        source_sequence_lengths:valid_sources_lengths,\n",
    "                        target_sequence_lengths:valid_targets_lengths\n",
    "                    })\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}  - Validation loss: {:>6.3f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(train_source) // batch_size, \n",
    "                              loss, \n",
    "                              val_loss[0]))\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, checkpoint)\n",
    "    print('Model Trained and Saved')          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x146af422f98>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhIAAAFkCAYAAAB1rtL+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQpIkVe3/BxTwSCHNNB38AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x146aac81da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "with open(\"letter_dicts.pkl\",'rb') as f:\n",
    "    source_int_to_letter, source_letter_to_int,target_int_to_letter, target_letter_to_int =pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def source_to_seq(text,sequence_length = 7):\n",
    "    return [source_letter_to_int.get(word, source_letter_to_int['<UNK>']) for word in text]+ [source_letter_to_int['<PAD>']]*(sequence_length-len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../data/letter_checkpoints/best_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "input_sentence = 'hello'\n",
    "text = source_to_seq(input_sentence)\n",
    "batch_size=128\n",
    "checkpoint = \"../data/letter_checkpoints/best_model.ckpt\" \n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    loader = tf.train.import_meta_graph(checkpoint+'.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "    inputs = loaded_graph.get_tensor_by_name('inputs:0')\n",
    "    preds = loaded_graph.get_tensor_by_name(\"predictions:0\")\n",
    "    source_sequence_lengths = loaded_graph.get_tensor_by_name('source_sequence_lengths:0')\n",
    "    target_sequence_lengths = loaded_graph.get_tensor_by_name('target_sequence_lengths:0')\n",
    "    \n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    answers = sess.run(preds, feed_dict={\n",
    "            inputs: [text]*batch_size,\n",
    "            source_sequence_lengths: [len(text)]*batch_size,\n",
    "            target_sequence_lengths: [len(text)]*batch_size\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 6)\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(answers.shape)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: hello\n",
      "\n",
      "Source\n",
      "  Word Ids:    [18, 4, 19, 19, 24, 0, 0]\n",
      "  Input Words: h e l l o <PAD> <PAD>\n",
      "\n",
      "Target\n",
      "  Word Ids:       [ 4 18 19 19 24  3]\n",
      "  Response Words: e h l l o <EOS>\n"
     ]
    }
   ],
   "source": [
    "pad = source_letter_to_int[\"<PAD>\"] \n",
    "print('Original Text:', input_sentence)\n",
    "\n",
    "print('\\nSource')\n",
    "print('  Word Ids:    {}'.format(text))\n",
    "print('  Input Words: {}'.format(\" \".join([source_int_to_letter[i] for i in text])))\n",
    "\n",
    "print('\\nTarget')\n",
    "print('  Word Ids:       {}'.format(answers[0]))\n",
    "print('  Response Words: {}'.format(\" \".join([target_int_to_letter[i] for i in answers[0] if i != pad])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
